{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Œ 1ï¸âƒ£ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 1ï¸âƒ£ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# âœ… 2ï¸âƒ£ EarlyStopping í´ë˜ìŠ¤ ì§ì ‘ ì •ì˜\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Œ 2ï¸âƒ£ ë°ì´í„° ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 1ï¸âƒ£ ë°ì´í„° ë¡œë“œ\n",
    "train_df = pd.read_csv(\"./data/train.csv\")\n",
    "test_df = pd.read_csv(\"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Œ 3ï¸âƒ£ TF-IDF ë²¡í„°í™” (ìµœì í™” ì ìš©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mí˜„ì¬ ì…€ ë˜ëŠ” ì´ì „ ì…€ì—ì„œ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ëŠ” ë™ì•ˆ Kernelì´ ì¶©ëŒí–ˆìŠµë‹ˆë‹¤. \n",
      "\u001b[1;31mì…€ì˜ ì½”ë“œë¥¼ ê²€í† í•˜ì—¬ ê°€ëŠ¥í•œ ì˜¤ë¥˜ ì›ì¸ì„ ì‹ë³„í•˜ì„¸ìš”. \n",
      "\u001b[1;31mìì„¸í•œ ë‚´ìš©ì„ ë³´ë ¤ë©´ <a href='https://aka.ms/vscodeJupyterKernelCrash'>ì—¬ê¸°</a>ë¥¼ í´ë¦­í•˜ì„¸ìš”. \n",
      "\u001b[1;31mìì„¸í•œ ë‚´ìš©ì€ Jupyter <a href='command:jupyter.viewOutput'>ë¡œê·¸</a>ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”."
     ]
    }
   ],
   "source": [
    "# âœ… 4ï¸âƒ£ TF-IDF ë²¡í„°í™” (ìµœì í™” ì ìš©)\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),   # âœ… 1~2-gram ì‚¬ìš©\n",
    "    max_features=1000,    # âœ… Feature ê°œìˆ˜ ì¡°ì •\n",
    "    sublinear_tf=True,    # âœ… TF ê°’ log ìŠ¤ì¼€ì¼ë§ ì ìš©\n",
    "    stop_words=None       # âœ… ë¶ˆìš©ì–´ ì œê±° ì•ˆ í•¨ (URLì— íŠ¹ìˆ˜ ë¬¸ì ë§ìŒ)\n",
    ")\n",
    "\n",
    "# âœ… í¬ì†Œ í–‰ë ¬ ì ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "train_tfidf = vectorizer.fit_transform(train_df[\"URL\"])\n",
    "test_tfidf = vectorizer.transform(test_df[\"URL\"])\n",
    "\n",
    "# âœ… í¬ì†Œ í–‰ë ¬ì„ ê·¸ëŒ€ë¡œ PyTorch Tensorë¡œ ë³€í™˜ (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "X = torch.tensor(train_tfidf.astype(np.float32).toarray(), dtype=torch.float32)\n",
    "y = torch.tensor(train_df[\"label\"].values, dtype=torch.float32)\n",
    "X_test = torch.tensor(test_tfidf.astype(np.float32).toarray(), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ğŸ“Œ 5ï¸âƒ£ ë°ì´í„°ì…‹ ë¶„ë¦¬ (Train 80% / Validation 20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Œ 6ï¸âƒ£ PyTorch Dataset ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 6ï¸âƒ£ PyTorch Dataset ì •ì˜\n",
    "class URLDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = X\n",
    "        self.y = y if y is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is not None:\n",
    "            return self.X[idx], self.y[idx]\n",
    "        return self.X[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Œ 7ï¸âƒ£ DataLoader ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 7ï¸âƒ£ DataLoader ì„¤ì • (ì†ë„ ìµœì í™”)\n",
    "batch_size = 128  # ê¸°ì¡´ 256ì—ì„œ ê°ì†Œ\n",
    "train_dataset = URLDataset(X_train, y_train)\n",
    "val_dataset = URLDataset(X_val, y_val)\n",
    "test_dataset = URLDataset(X_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Œ 8ï¸âƒ£ MLP ê¸°ë°˜ URL ë¶„ë¥˜ ëª¨ë¸ (êµ¬ì¡° ê°œì„ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 8ï¸âƒ£ ë” ê¹Šì€ MLP ê¸°ë°˜ URL ë¶„ë¥˜ ëª¨ë¸\n",
    "class DeepURLClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeepURLClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.fc5 = nn.Linear(128, 1)  # Output Layer\n",
    "\n",
    "        self.dropout = nn.Dropout(0.4)  # âœ… ê¸°ì¡´ë³´ë‹¤ ë†’ì€ Dropout ì ìš©\n",
    "        self.swish = nn.SiLU()  # âœ… Swish í™œì„± í•¨ìˆ˜ ì ìš©\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.swish(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.swish(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.swish(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.swish(self.bn4(self.fc4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sigmoid(self.fc5(x))  # âœ… ë§ˆì§€ë§‰ ì¸µì—ì„œëŠ” ì‹œê·¸ëª¨ì´ë“œ ì ìš©\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Œ 1ï¸âƒ£1ï¸âƒ£ ëª¨ë¸ ì´ˆê¸°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… ëª¨ë¸ ì´ˆê¸°í™”\n",
    "input_dim = X_train.shape[1]\n",
    "model = DeepURLClassifier(input_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Œ 1ï¸âƒ£2ï¸âƒ£ Optimizer & Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… ğŸ”Ÿ Optimizer & Loss ì„¤ì •\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Œ 1ï¸âƒ£3ï¸âƒ£ í•™ìŠµ í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: cuda:0\n",
      "Epoch 1 ì‹œì‘...\n",
      "âœ… Epoch 1 ì™„ë£Œ! Train Loss: 0.2590\n",
      "ğŸ“Š Epoch 1 - Validation Loss: 0.2493 | ROC-AUC: 0.9066\n",
      "Validation loss decreased (inf --> 0.249335).  Saving model ...\n",
      "Epoch 2 ì‹œì‘...\n",
      "âœ… Epoch 2 ì™„ë£Œ! Train Loss: 0.2505\n",
      "ğŸ“Š Epoch 2 - Validation Loss: 0.2467 | ROC-AUC: 0.9079\n",
      "Validation loss decreased (0.249335 --> 0.246676).  Saving model ...\n",
      "Epoch 3 ì‹œì‘...\n",
      "âœ… Epoch 3 ì™„ë£Œ! Train Loss: 0.2484\n",
      "ğŸ“Š Epoch 3 - Validation Loss: 0.2460 | ROC-AUC: 0.9083\n",
      "Validation loss decreased (0.246676 --> 0.245996).  Saving model ...\n",
      "Epoch 4 ì‹œì‘...\n",
      "âœ… Epoch 4 ì™„ë£Œ! Train Loss: 0.2474\n",
      "ğŸ“Š Epoch 4 - Validation Loss: 0.2453 | ROC-AUC: 0.9086\n",
      "Validation loss decreased (0.245996 --> 0.245335).  Saving model ...\n",
      "Epoch 5 ì‹œì‘...\n",
      "âœ… Epoch 5 ì™„ë£Œ! Train Loss: 0.2466\n",
      "ğŸ“Š Epoch 5 - Validation Loss: 0.2451 | ROC-AUC: 0.9087\n",
      "Validation loss decreased (0.245335 --> 0.245062).  Saving model ...\n",
      "Epoch 6 ì‹œì‘...\n",
      "âœ… Epoch 6 ì™„ë£Œ! Train Loss: 0.2461\n",
      "ğŸ“Š Epoch 6 - Validation Loss: 0.2450 | ROC-AUC: 0.9086\n",
      "Validation loss decreased (0.245062 --> 0.245023).  Saving model ...\n",
      "Epoch 7 ì‹œì‘...\n",
      "âœ… Epoch 7 ì™„ë£Œ! Train Loss: 0.2457\n",
      "ğŸ“Š Epoch 7 - Validation Loss: 0.2443 | ROC-AUC: 0.9091\n",
      "Validation loss decreased (0.245023 --> 0.244281).  Saving model ...\n",
      "Epoch 8 ì‹œì‘...\n",
      "âœ… Epoch 8 ì™„ë£Œ! Train Loss: 0.2454\n",
      "ğŸ“Š Epoch 8 - Validation Loss: 0.2447 | ROC-AUC: 0.9089\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Epoch 9 ì‹œì‘...\n",
      "âœ… Epoch 9 ì™„ë£Œ! Train Loss: 0.2451\n",
      "ğŸ“Š Epoch 9 - Validation Loss: 0.2446 | ROC-AUC: 0.9089\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Epoch 10 ì‹œì‘...\n",
      "âœ… Epoch 10 ì™„ë£Œ! Train Loss: 0.2449\n",
      "ğŸ“Š Epoch 10 - Validation Loss: 0.2440 | ROC-AUC: 0.9091\n",
      "Validation loss decreased (0.244281 --> 0.243995).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "# âœ… 1ï¸âƒ£1ï¸âƒ£ ë””ë°”ì´ìŠ¤ ì„¤ì • (M1 ë§¥ë¶ì—ì„œëŠ” CPU ê°•ì œ ì‚¬ìš©)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}\")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# âœ… 1ï¸âƒ£2ï¸âƒ£ í•™ìŠµ í•¨ìˆ˜ ì •ì˜ (ROC-AUC ì¶œë ¥ ì¶”ê°€)\n",
    "def train_model(model, train_loader, val_loader, epochs=10):\n",
    "    early_stopping = EarlyStopping(patience=3, verbose=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        print(f\"Epoch {epoch+1} ì‹œì‘...\")\n",
    "\n",
    "        # ğŸ”¹ ğŸ”¥ Training Loop\n",
    "        # for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "        #     X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        #     optimizer.zero_grad()\n",
    "        #     outputs = model(X_batch).squeeze()\n",
    "        #     loss = criterion(outputs, y_batch)\n",
    "        #     loss.backward()\n",
    "        #     optimizer.step()\n",
    "        #     train_loss += loss.item()\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # ë°ì´í„° ë””ë°”ì´ìŠ¤ ì´ë™\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward, Loss, Backward\n",
    "            outputs = model(X_batch).squeeze()  # âœ… squeeze()ë¡œ ì°¨ì› ì¶•ì†Œ\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "\n",
    "            # if batch_idx % 1000 == 0:\n",
    "            #     print(f\"  ğŸ”„ [Batch {batch_idx}/{len(train_loader)}] Train Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # âœ… Training ì™„ë£Œ í›„ í‰ê·  Loss ì¶œë ¥\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        print(f\"âœ… Epoch {epoch+1} ì™„ë£Œ! Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # ğŸ”¹ ğŸ”¥ Validation Loop (ROC-AUC ì ìˆ˜ ê³„ì‚°)\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        y_true, y_pred = [], []  # âœ… ì‹¤ì œê°’ & ì˜ˆì¸¡ê°’ ì €ì¥ ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch).squeeze()\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                y_true.extend(y_batch.cpu().numpy())  # âœ… ì‹¤ì œê°’ ì €ì¥\n",
    "                y_pred.extend(outputs.cpu().numpy())  # âœ… ì˜ˆì¸¡ê°’ ì €ì¥\n",
    "\n",
    "        # âœ… Validation Loss & ROC-AUC ê³„ì‚° \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        roc_auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "        print(f\"ğŸ“Š Epoch {epoch+1} - Validation Loss: {avg_val_loss:.4f} | ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "        # âœ… Early Stopping ì ìš©\n",
    "        early_stopping(avg_val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"âœ… ì¡°ê¸° ì¢…ë£Œ (Early Stopping)\")\n",
    "            break\n",
    "\n",
    "# âœ… 1ï¸âƒ£3ï¸âƒ£ í•™ìŠµ ì‹¤í–‰\n",
    "train_model(model, train_loader, val_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ìµœì¢… ì˜ˆì¸¡ ì™„ë£Œ. ì œì¶œ íŒŒì¼ ìƒì„±ë¨!\n"
     ]
    }
   ],
   "source": [
    "# âœ… 1ï¸âƒ£4ï¸âƒ£ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "model.eval()\n",
    "y_test_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = model(X_batch).squeeze()\n",
    "        y_test_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "submission = pd.DataFrame({\"ID\": test_df[\"ID\"], \"probability\": y_test_preds})\n",
    "submission.to_csv(\"submission_6th.csv\", index=False)\n",
    "print(\"âœ… ìµœì¢… ì˜ˆì¸¡ ì™„ë£Œ. ì œì¶œ íŒŒì¼ ìƒì„±ë¨!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malicious_url_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
