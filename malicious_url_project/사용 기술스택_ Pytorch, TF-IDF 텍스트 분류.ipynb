{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📌 1️⃣ 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 1️⃣ 라이브러리 로드\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ✅ 2️⃣ EarlyStopping 클래스 직접 정의\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📌 2️⃣ 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 1️⃣ 데이터 로드\n",
    "train_df = pd.read_csv(\"./data/train.csv\")\n",
    "test_df = pd.read_csv(\"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📌 3️⃣ TF-IDF 벡터화 (최적화 적용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "# ✅ 4️⃣ TF-IDF 벡터화 (최적화 적용)\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),   # ✅ 1~2-gram 사용\n",
    "    max_features=1000,    # ✅ Feature 개수 조정\n",
    "    sublinear_tf=True,    # ✅ TF 값 log 스케일링 적용\n",
    "    stop_words=None       # ✅ 불용어 제거 안 함 (URL에 특수 문자 많음)\n",
    ")\n",
    "\n",
    "# ✅ 희소 행렬 적용하여 메모리 절약\n",
    "train_tfidf = vectorizer.fit_transform(train_df[\"URL\"])\n",
    "test_tfidf = vectorizer.transform(test_df[\"URL\"])\n",
    "\n",
    "# ✅ 희소 행렬을 그대로 PyTorch Tensor로 변환 (메모리 절약)\n",
    "X = torch.tensor(train_tfidf.astype(np.float32).toarray(), dtype=torch.float32)\n",
    "y = torch.tensor(train_df[\"label\"].values, dtype=torch.float32)\n",
    "X_test = torch.tensor(test_tfidf.astype(np.float32).toarray(), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 📌 5️⃣ 데이터셋 분리 (Train 80% / Validation 20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📌 6️⃣ PyTorch Dataset 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 6️⃣ PyTorch Dataset 정의\n",
    "class URLDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = X\n",
    "        self.y = y if y is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is not None:\n",
    "            return self.X[idx], self.y[idx]\n",
    "        return self.X[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📌 7️⃣ DataLoader 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 7️⃣ DataLoader 설정 (속도 최적화)\n",
    "batch_size = 128  # 기존 256에서 감소\n",
    "train_dataset = URLDataset(X_train, y_train)\n",
    "val_dataset = URLDataset(X_val, y_val)\n",
    "test_dataset = URLDataset(X_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📌 8️⃣ MLP 기반 URL 분류 모델 (구조 개선)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 8️⃣ 더 깊은 MLP 기반 URL 분류 모델\n",
    "class DeepURLClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeepURLClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.fc5 = nn.Linear(128, 1)  # Output Layer\n",
    "\n",
    "        self.dropout = nn.Dropout(0.4)  # ✅ 기존보다 높은 Dropout 적용\n",
    "        self.swish = nn.SiLU()  # ✅ Swish 활성 함수 적용\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.swish(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.swish(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.swish(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.swish(self.bn4(self.fc4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sigmoid(self.fc5(x))  # ✅ 마지막 층에서는 시그모이드 적용\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📌 1️⃣1️⃣ 모델 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 모델 초기화\n",
    "input_dim = X_train.shape[1]\n",
    "model = DeepURLClassifier(input_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📌 1️⃣2️⃣ Optimizer & Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 🔟 Optimizer & Loss 설정\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📌 1️⃣3️⃣ 학습 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 중인 디바이스: cuda:0\n",
      "Epoch 1 시작...\n",
      "✅ Epoch 1 완료! Train Loss: 0.2590\n",
      "📊 Epoch 1 - Validation Loss: 0.2493 | ROC-AUC: 0.9066\n",
      "Validation loss decreased (inf --> 0.249335).  Saving model ...\n",
      "Epoch 2 시작...\n",
      "✅ Epoch 2 완료! Train Loss: 0.2505\n",
      "📊 Epoch 2 - Validation Loss: 0.2467 | ROC-AUC: 0.9079\n",
      "Validation loss decreased (0.249335 --> 0.246676).  Saving model ...\n",
      "Epoch 3 시작...\n",
      "✅ Epoch 3 완료! Train Loss: 0.2484\n",
      "📊 Epoch 3 - Validation Loss: 0.2460 | ROC-AUC: 0.9083\n",
      "Validation loss decreased (0.246676 --> 0.245996).  Saving model ...\n",
      "Epoch 4 시작...\n",
      "✅ Epoch 4 완료! Train Loss: 0.2474\n",
      "📊 Epoch 4 - Validation Loss: 0.2453 | ROC-AUC: 0.9086\n",
      "Validation loss decreased (0.245996 --> 0.245335).  Saving model ...\n",
      "Epoch 5 시작...\n",
      "✅ Epoch 5 완료! Train Loss: 0.2466\n",
      "📊 Epoch 5 - Validation Loss: 0.2451 | ROC-AUC: 0.9087\n",
      "Validation loss decreased (0.245335 --> 0.245062).  Saving model ...\n",
      "Epoch 6 시작...\n",
      "✅ Epoch 6 완료! Train Loss: 0.2461\n",
      "📊 Epoch 6 - Validation Loss: 0.2450 | ROC-AUC: 0.9086\n",
      "Validation loss decreased (0.245062 --> 0.245023).  Saving model ...\n",
      "Epoch 7 시작...\n",
      "✅ Epoch 7 완료! Train Loss: 0.2457\n",
      "📊 Epoch 7 - Validation Loss: 0.2443 | ROC-AUC: 0.9091\n",
      "Validation loss decreased (0.245023 --> 0.244281).  Saving model ...\n",
      "Epoch 8 시작...\n",
      "✅ Epoch 8 완료! Train Loss: 0.2454\n",
      "📊 Epoch 8 - Validation Loss: 0.2447 | ROC-AUC: 0.9089\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Epoch 9 시작...\n",
      "✅ Epoch 9 완료! Train Loss: 0.2451\n",
      "📊 Epoch 9 - Validation Loss: 0.2446 | ROC-AUC: 0.9089\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Epoch 10 시작...\n",
      "✅ Epoch 10 완료! Train Loss: 0.2449\n",
      "📊 Epoch 10 - Validation Loss: 0.2440 | ROC-AUC: 0.9091\n",
      "Validation loss decreased (0.244281 --> 0.243995).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "# ✅ 1️⃣1️⃣ 디바이스 설정 (M1 맥북에서는 CPU 강제 사용)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"사용 중인 디바이스: {device}\")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# ✅ 1️⃣2️⃣ 학습 함수 정의 (ROC-AUC 출력 추가)\n",
    "def train_model(model, train_loader, val_loader, epochs=10):\n",
    "    early_stopping = EarlyStopping(patience=3, verbose=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        print(f\"Epoch {epoch+1} 시작...\")\n",
    "\n",
    "        # 🔹 🔥 Training Loop\n",
    "        # for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "        #     X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        #     optimizer.zero_grad()\n",
    "        #     outputs = model(X_batch).squeeze()\n",
    "        #     loss = criterion(outputs, y_batch)\n",
    "        #     loss.backward()\n",
    "        #     optimizer.step()\n",
    "        #     train_loss += loss.item()\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # 데이터 디바이스 이동\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # 옵티마이저 초기화\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward, Loss, Backward\n",
    "            outputs = model(X_batch).squeeze()  # ✅ squeeze()로 차원 축소\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "\n",
    "            # if batch_idx % 1000 == 0:\n",
    "            #     print(f\"  🔄 [Batch {batch_idx}/{len(train_loader)}] Train Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # ✅ Training 완료 후 평균 Loss 출력\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        print(f\"✅ Epoch {epoch+1} 완료! Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # 🔹 🔥 Validation Loop (ROC-AUC 점수 계산)\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        y_true, y_pred = [], []  # ✅ 실제값 & 예측값 저장 리스트\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch).squeeze()\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                y_true.extend(y_batch.cpu().numpy())  # ✅ 실제값 저장\n",
    "                y_pred.extend(outputs.cpu().numpy())  # ✅ 예측값 저장\n",
    "\n",
    "        # ✅ Validation Loss & ROC-AUC 계산 \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        roc_auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "        print(f\"📊 Epoch {epoch+1} - Validation Loss: {avg_val_loss:.4f} | ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "        # ✅ Early Stopping 적용\n",
    "        early_stopping(avg_val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"✅ 조기 종료 (Early Stopping)\")\n",
    "            break\n",
    "\n",
    "# ✅ 1️⃣3️⃣ 학습 실행\n",
    "train_model(model, train_loader, val_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 최종 예측 완료. 제출 파일 생성됨!\n"
     ]
    }
   ],
   "source": [
    "# ✅ 1️⃣4️⃣ 테스트 데이터 예측 및 제출 파일 생성\n",
    "model.eval()\n",
    "y_test_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = model(X_batch).squeeze()\n",
    "        y_test_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "submission = pd.DataFrame({\"ID\": test_df[\"ID\"], \"probability\": y_test_preds})\n",
    "submission.to_csv(\"submission_6th.csv\", index=False)\n",
    "print(\"✅ 최종 예측 완료. 제출 파일 생성됨!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malicious_url_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
